\section{Data Acquisition}
\label{sec:data_acquisition}

The dataset used for this study comes from the Inside Airbnb website
\textcite{Airbnb}. This independent initiative collects Airbnb data for more than 30
major cities around the world that scrapes Airbnb listings, reviews, and
calendar data from multiple cities worldwide. The dataset  was scraped on
December 4, 2019, contains information about the location (latitude and
longitude coordinates) of all Airbnb listings in New York City and data about
the hostname and ID, room type, price, minimum nights, number of reviews
listings per host, and availability... A GeoJSON file of New York borough
boundaries was also downloaded from the same site.

The raw data is quite untidy and has some weaknesses. The major one is that it
only includes the advertised price (sometimes called the ‘sticker’ price). The
sticker price is the overall nightly price advertised to potential renters,
rather than the actual average amount paid per night by previous guests. A host
can set the advertised prices to any arbitrary amount, and those that do not
have much experience with Airbnb will often set these to unreasonable prices,
such as too low (e.g., \$0) or very high (e.g., \$10,000) amounts.

These variables are listed and defined in Table ~\ref{tab:variable-list}.

\section{Data Pre-Processing }
\label{sec:data_cleaning}

Real-world data is often dirty; that is, it is in need of being cleaned up
before it can be used for the desired purpose such as visualization, model
building. This is often called data pre-processing.

Since there are several reasons why data could be “dirty,” there are just as
various techniques to “clean” it.  For this analysis, we will take a look at
three key methods that illustrate ways in which data may be “cleaned,” or better
organized, or scrubbed of potentially incorrect, incomplete, or duplicated data.

\subsection{Data Filtering}

Following \textcite{wang2017price}, this study filtered the listings with at
least one online customer review to guarantee that our sample listings were
"active." This view is supported by \textcite{ye2009impact}  which confirms that online
review ratings are associated with hotel-room sales, indicating that reviews
suggest real transactions.

\subsection{Removing Predictors}

There are three reasons why we should consider removing predictors before
modeling. First, fitting a model with fewer predictors reduces computational
time and complexity. Second, removing highly correlated features makes the model
more parsimonious and interpretable model.  Lastly, some models can be crippled
by predictors with degenerate distributions.  Therefore, eliminating the
problematic predictors prior to fitting a model can significantly improve model
performance and stability.
\begin{itemize}

    \item We remove  predictors with a single unique value as they are  zero
        variance predictors. These predictors are \texttt{has\_availability},
        \texttt{host\_has\_profile\_pic},\texttt{requires\_license},
        \texttt{is\_business\_travel\_ready},
        \texttt{require\_guest\_phone\_verification},\newline
        \texttt{require\_guest\_profile\_picture} (See
        \ref{fig:histogram-feature-distribution}) .
    \item It is beyond the scope of this study to explore the natural language
        processing for predictive modeling.  Therefore,  we will drop free-text
        columns for now, as will other columns that are not useful for
        predicting price (e.g., \texttt{url}, \texttt{hostname}, and other host-related features
        unrelated to the property).

    \item We drop feature which  adds relatively little information, or are
        relative unhelpful in differentiating between different listings.

\end{itemize}

\subsection{Dealing with Missing Value}

Missing data is a common issue in many data analysis applications.  Data may be
missing due to problems with the process of collecting data or equipment
malfunction. Some data may get lost due to system or human error while storing
or transferring the data.

There are two approaches we take to handle missing
data:
\begin{enumerate}
    \item Filling out missing value :  we drop the predictors
        that contain a majority of null entries as in Table
        ~\ref{tab:missing-value}.
    \item Filling in missing value:we perform data imputation, which means
        filling the missing data with some estimated ones. In particular,

        \begin{itemize}

            \item Missing values in numeric features such as the number of
                bathrooms (\texttt{bathrooms}) bathrooms, the number of bedrooms
                (\texttt{bedrooms}), the number of beds (\texttt{beds}) will be
                replaced with the median.

            \item For monetary features such as the amount required as a
                security deposit (\texttt{security\_deposit}), the amount of the cleaning
                fee (\texttt{cleaning\_fee}), the price per additional guest
                (extra\_people), having a missing value for them is the same as
                having the value of \$0, so the missing values will be replaced
                with 0. Similarly, an amenity that has a missing value will be replaced with 0.

        \end{itemize}
\end{enumerate}

\begin{table}[htp]
    \centering
    \caption{Columns with majority of null values}
    \label{tab:missing-value}
{\small
\begin{tabular}{lr}
\toprule
{Column} &      Number of missing value \\
\midrule
host\_acceptance\_rate &  50599 \\
jurisdiction\_names   &  50583 \\
license              &  50577 \\
square\_feet          &  50213 \\
monthly\_price        &  45683 \\
weekly\_price         &  44945 \\
\bottomrule
\end{tabular}

}
\end{table}

\subsection{Feature Construction}

\begin{itemize}

    \item We convert DateTime columns such as the date that the host first joined Airbnb
        (\texttt{host\_since}) to measure the number of days a host has been on
        the platform, measured from the date that the data was collected
        (\texttt{host\_days\_active}).

    \item We create a new feature that measures the number of days between the first
        review and  the date we collect the data
        (\texttt{time\_since\_first\_review}) from
        the feature the date of the first review (\texttt{first\_review}).

    \item We can compute the number of days between the most recent review and
        the date the data was scraped (\texttt{time\_since\_last\_review}) using the date of the
        most recent review (\texttt{last\_review}).
\end{itemize}

\subsection{Binning Predictors}

We want to separate continuous data into "bins" for analysis in many situations, especially when that data has a wide range.
Binning, also known as quantization, is a useful technique for converting continuous numeric features into discrete ones (categories).
In our dataset, we peform the following binning:
\begin{itemize}

    \item The feature that measures proportion of messages that the host replies
(host\_response\_rate) will be bin into four categories '0-49\%', '50-89\%',
'90-99\%', '100\%',

    \item The number of days between the first review and the date we collect the data
        (\texttt{time\_since\_first\_review}) will be bin into '0-6 months', '6-12 months', '1-2
    years', '2-3 years', '4+ years'

    \item The number of days between the most recent review and the date the data was
        scraped (\texttt{time\_since\_last\_review}) will be bin into '0-2 weeks','2-8 weeks',
    '2-6 months', '6-12 months', '1+ year'

    \item Review scores rating (\texttt{review\_scores\_rating}) will be bin into following
        categories '0-79/100', '80-94/100', '95-100/100'
\end{itemize}



\subsection{Data Transformation}

Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod
tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At
vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren,
no sea takimata sanctus est Lorem ipsum dolor sit amet.

\subsubsection{Feature Scaling}

Feature scaling is an essential step in the preprocessing process, as some
machine learning algorithms do not perform well when the numerical input
attributes have very different scales.

Feature scaling through standardization (or Z-score normalization) is straightforward and
common-used in the machine learning community.
Standardization means that we rescale the predictors so that they have a zero
mean and standard deviation 1.  We first compute the mean and standard deviation
for the feature and scale it based on:
\[x^{'} = \frac{x - \bar{x}}{\sigma}\]
where $x$ is the original feature vector, $\bar{x} = \textrm{average(x)}$ is the mean of
that feature vector, and $\sigma$ is its standard deviation.

\noindent In our dataset we use  a transformer called StandardScaler from Scikit-Learn for
standardization.

\subsubsection{Transformation to Resolve Skewness}

Many models assume a normal distribution, which means data are symmetric about
the mean.
Unfortunately, our real-life datasets do not always follow the normal
distribution.  Instead, they are usually skewed, which makes the results of our
statistical analyses invalid.

Figure (Insert Figure) shows the distribution of numerical features. We notice
that most of the predictors have a tail-heavy distribution. Therefore we
transform them by replacing them with their logarithm. Figure (Insert Figure)
shows the result; some of the distributions become normal distributive. More
importantly, the outcome variable price appears much more normally distributed.

\subsection{Handling Categorical Attributes}

One of the significant problems with machine learning is that many algorithms
cannot work directly with categorical data (non-numerical values).  Hence, we
need a way to convert categorical data into a numerical form, and our machine
learning algorithm can take in that as input. In this study, we use  One-Hot
Encoding, one of the most widely used encoding techniques.  One-hot encoding is
processed in 2 steps:
\begin{enumerate}
    \item First, we split categories into different columns.
    \item We then put 0 for others and 1 as an indicator for the appropriate column.
\end{enumerate}

\noindent Pandas Library provides  the \texttt{get\_dummies()} method to
perform One-Hot enconding

